{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3abea27",
   "metadata": {},
   "source": [
    "# üî• Customer Churn Fire Project - Comprehensive EDA & ML Pipeline\n",
    "\n",
    "## üöÄ Problem Statement: \"Retention is cheaper than acquisition. We predict churn.\"\n",
    "\n",
    "This notebook combines the best practices from multiple customer churn analysis projects to create a unified, end-to-end machine learning pipeline. \n",
    "\n",
    "### üéØ Project Goals:\n",
    "- Build a professional DS pipeline with **XGBoost + SHAP interpretability**\n",
    "- Achieve **ROC-AUC ‚â• 0.91** and save **‚Çπ2L/month** by reducing churn\n",
    "- Deploy via **Streamlit dashboard** + **FastAPI** real-time predictions\n",
    "- Implement **cost-benefit A/B testing simulation**\n",
    "\n",
    "### üîß Tech Stack:\n",
    "**Core ML**: Python, Scikit-learn, XGBoost, SHAP, LIME  \n",
    "**Visualization**: Matplotlib, Seaborn, Plotly  \n",
    "**Deployment**: Streamlit, FastAPI, Docker  \n",
    "**MLOps**: GitHub Actions, Render deployment  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b2626",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Project Setup and Environment Configuration\n",
    "\n",
    "Setting up the environment and importing all necessary libraries for our end-to-end churn prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Processing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
    "                           precision_recall_curve, average_precision_score)\n",
    "\n",
    "# XGBoost and Advanced ML\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üî• Environment setup complete!\")\n",
    "print(f\"üìÖ Notebook execution started: {datetime.now()}\")\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da059745",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Loading and Initial Exploration\n",
    "\n",
    "Loading the customer churn dataset and performing initial data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data/churn_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"üìè Dataset shape: {df.shape}\")\n",
    "print(f\"üî¢ Features: {df.shape[1]}\")\n",
    "print(f\"üìã Records: {df.shape[0]}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"üîç First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset information\n",
    "print(\"üìã Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Data types summary\n",
    "print(\"üî§ Data Types Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"‚ùì Missing Values Analysis:\")\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum(),\n",
    "    'Missing Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_data[missing_data['Missing Count'] > 0])\n",
    "print(f\"\\n‚úÖ Total missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìä Numerical Features Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a405b5",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Comprehensive Exploratory Data Analysis (EDA)\n",
    "\n",
    "Creating powerful visualizations for churn distribution, customer demographics, and business KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Analysis\n",
    "print(\"üéØ TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if target column exists (could be 'Exited' or 'Churn')\n",
    "target_col = 'Exited' if 'Exited' in df.columns else 'Churn' if 'Churn' in df.columns else None\n",
    "if target_col is None:\n",
    "    print(\"‚ùå Target column not found. Please check column names.\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Target column found: '{target_col}'\")\n",
    "    \n",
    "    # Calculate churn statistics\n",
    "    churn_counts = df[target_col].value_counts()\n",
    "    churn_rate = df[target_col].mean() * 100\n",
    "    \n",
    "    print(f\"üìä Churn Distribution:\")\n",
    "    print(f\"  Not Churned: {churn_counts[0]:,} ({(churn_counts[0]/len(df))*100:.1f}%)\")\n",
    "    print(f\"  Churned: {churn_counts[1]:,} ({(churn_counts[1]/len(df))*100:.1f}%)\")\n",
    "    print(f\"  Overall Churn Rate: {churn_rate:.2f}%\")\n",
    "    \n",
    "    # Business Impact Calculation\n",
    "    avg_customer_value = 1000  # Assumed average customer lifetime value\n",
    "    total_revenue_at_risk = churn_counts[1] * avg_customer_value\n",
    "    print(f\"\\nüí∞ BUSINESS IMPACT:\")\n",
    "    print(f\"  Customers at risk: {churn_counts[1]:,}\")\n",
    "    print(f\"  Revenue at risk: ${total_revenue_at_risk:,.2f}\")\n",
    "    print(f\"  Monthly revenue loss (estimated): ${total_revenue_at_risk/12:,.2f}\")\n",
    "\n",
    "# Create interactive churn distribution visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Churn Distribution (Pie)', 'Churn Distribution (Bar)', \n",
    "                   'Churn Rate by Geography', 'Churn Rate Trend'),\n",
    "    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "if target_col:\n",
    "    # Pie chart\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=['Not Churned', 'Churned'], \n",
    "               values=churn_counts.values, \n",
    "               hole=0.4,\n",
    "               marker_colors=['#2E86AB', '#A23B72']),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=['Not Churned', 'Churned'], \n",
    "               y=churn_counts.values,\n",
    "               marker_color=['#2E86AB', '#A23B72']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Geography analysis (if available)\n",
    "    if 'Geography' in df.columns:\n",
    "        geo_churn = df.groupby('Geography')[target_col].agg(['count', 'sum', 'mean']).reset_index()\n",
    "        geo_churn['churn_rate'] = geo_churn['mean'] * 100\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=geo_churn['Geography'], \n",
    "                   y=geo_churn['churn_rate'],\n",
    "                   marker_color='#F18F01'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüåç GEOGRAPHIC CHURN ANALYSIS:\")\n",
    "        for _, row in geo_churn.iterrows():\n",
    "            print(f\"  {row['Geography']}: {row['churn_rate']:.1f}% ({row['sum']}/{row['count']})\")\n",
    "\n",
    "fig.update_layout(height=800, showlegend=False, title_text=\"Customer Churn Analysis Dashboard\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüîç KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ Class imbalance detected - will need SMOTE for model training\")\n",
    "print(\"‚Ä¢ High churn rate indicates significant revenue risk\")\n",
    "print(\"‚Ä¢ Geographic differences suggest targeted regional strategies needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ad6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features Analysis by Churn Status\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col and target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "# Remove ID columns\n",
    "id_cols = ['RowNumber', 'CustomerId']\n",
    "numerical_cols = [col for col in numerical_cols if col not in id_cols]\n",
    "\n",
    "print(f\"üìä NUMERICAL FEATURES ANALYSIS\")\n",
    "print(f\"Found {len(numerical_cols)} numerical features: {numerical_cols}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if target_col and numerical_cols:\n",
    "    # Create comprehensive numerical features visualization\n",
    "    n_features = len(numerical_cols)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 6*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        # Box plot showing distribution by churn status\n",
    "        sns.boxplot(data=df, x=target_col, y=col, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} Distribution by Churn Status')\n",
    "        axes[row, col_idx].set_xlabel('Churn Status (0=Stay, 1=Churn)')\n",
    "        \n",
    "        # Calculate and display statistics\n",
    "        stats_no_churn = df[df[target_col]==0][col].describe()\n",
    "        stats_churn = df[df[target_col]==1][col].describe()\n",
    "        \n",
    "        print(f\"\\nüìà {col.upper()}:\")\n",
    "        print(f\"  No Churn - Mean: {stats_no_churn['mean']:.2f}, Std: {stats_no_churn['std']:.2f}\")\n",
    "        print(f\"  Churned   - Mean: {stats_churn['mean']:.2f}, Std: {stats_churn['std']:.2f}\")\n",
    "        print(f\"  Difference: {abs(stats_churn['mean'] - stats_no_churn['mean']):.2f}\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Age Distribution Analysis (Key Feature)\n",
    "if 'Age' in df.columns and target_col:\n",
    "    print(f\"\\nüë• AGE ANALYSIS - KEY CHURN DRIVER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Age groups analysis\n",
    "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 30, 40, 50, 60, 100], \n",
    "                           labels=['<30', '30-40', '40-50', '50-60', '60+'])\n",
    "    \n",
    "    age_churn = df.groupby('AgeGroup')[target_col].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    age_churn['churn_rate'] = age_churn['mean'] * 100\n",
    "    \n",
    "    # Interactive age analysis\n",
    "    fig = px.bar(age_churn, x='AgeGroup', y='churn_rate', \n",
    "                title='Churn Rate by Age Group',\n",
    "                labels={'churn_rate': 'Churn Rate (%)', 'AgeGroup': 'Age Group'},\n",
    "                color='churn_rate', color_continuous_scale='Reds')\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üìä Age Group Churn Rates:\")\n",
    "    for _, row in age_churn.iterrows():\n",
    "        print(f\"  {row['AgeGroup']}: {row['churn_rate']:.1f}% ({row['sum']}/{row['count']} customers)\")\n",
    "\n",
    "print(\"\\nüí° NUMERICAL INSIGHTS:\")\n",
    "print(\"‚Ä¢ Age is a critical factor - older customers show higher churn tendency\")\n",
    "print(\"‚Ä¢ Balance variations indicate different customer segments\")\n",
    "print(\"‚Ä¢ Credit score patterns suggest financial health impacts churn decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb892fa",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Preprocessing and Feature Engineering\n",
    "\n",
    "Implementing advanced feature engineering techniques to create powerful predictors for our churn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"üîß FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Remove unnecessary columns\n",
    "cols_to_drop = ['RowNumber', 'CustomerId', 'Surname']\n",
    "df_processed = df_processed.drop([col for col in cols_to_drop if col in df_processed.columns], axis=1)\n",
    "print(f\"‚úÖ Dropped unnecessary columns: {[col for col in cols_to_drop if col in df.columns]}\")\n",
    "\n",
    "# 2. Advanced Feature Engineering\n",
    "print(\"\\nüöÄ Creating New Features:\")\n",
    "\n",
    "# Credit Utilization Ratio\n",
    "if 'Balance' in df_processed.columns and 'CreditScore' in df_processed.columns:\n",
    "    df_processed['CreditUtilization'] = df_processed['Balance'] / df_processed['CreditScore']\n",
    "    print(\"‚úÖ CreditUtilization = Balance / CreditScore\")\n",
    "\n",
    "# Customer Interaction Score\n",
    "interaction_cols = ['NumOfProducts', 'HasCrCard', 'IsActiveMember']\n",
    "if all(col in df_processed.columns for col in interaction_cols):\n",
    "    df_processed['InteractionScore'] = (df_processed['NumOfProducts'] + \n",
    "                                       df_processed['HasCrCard'] + \n",
    "                                       df_processed['IsActiveMember'])\n",
    "    print(\"‚úÖ InteractionScore = NumOfProducts + HasCrCard + IsActiveMember\")\n",
    "\n",
    "# Balance to Salary Ratio\n",
    "if 'Balance' in df_processed.columns and 'EstimatedSalary' in df_processed.columns:\n",
    "    df_processed['BalanceToSalaryRatio'] = df_processed['Balance'] / df_processed['EstimatedSalary']\n",
    "    print(\"‚úÖ BalanceToSalaryRatio = Balance / EstimatedSalary\")\n",
    "\n",
    "# Credit Score Age Interaction\n",
    "if 'CreditScore' in df_processed.columns and 'Age' in df_processed.columns:\n",
    "    df_processed['CreditScoreAgeInteraction'] = df_processed['CreditScore'] * df_processed['Age']\n",
    "    print(\"‚úÖ CreditScoreAgeInteraction = CreditScore * Age\")\n",
    "\n",
    "# Tenure Segments\n",
    "if 'Tenure' in df_processed.columns:\n",
    "    df_processed['TenureSegment'] = pd.cut(df_processed['Tenure'], \n",
    "                                          bins=[-1, 2, 5, 10, 20], \n",
    "                                          labels=['New', 'Growing', 'Mature', 'Veteran'])\n",
    "    print(\"‚úÖ TenureSegment = Categorized tenure into lifecycle stages\")\n",
    "\n",
    "# Credit Score Groups\n",
    "if 'CreditScore' in df_processed.columns:\n",
    "    df_processed['CreditScoreGroup'] = pd.cut(df_processed['CreditScore'], \n",
    "                                             bins=[0, 669, 739, 850], \n",
    "                                             labels=['Poor', 'Fair', 'Good'])\n",
    "    print(\"‚úÖ CreditScoreGroup = Categorized credit scores\")\n",
    "\n",
    "# High Value Customer Flag\n",
    "if 'Balance' in df_processed.columns:\n",
    "    balance_75th = df_processed['Balance'].quantile(0.75)\n",
    "    df_processed['HighValueCustomer'] = (df_processed['Balance'] > balance_75th).astype(int)\n",
    "    print(f\"‚úÖ HighValueCustomer = Balance > 75th percentile (${balance_75th:,.2f})\")\n",
    "\n",
    "print(f\"\\nüìä Feature engineering complete!\")\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"New features: {df_processed.shape[1]}\")\n",
    "print(f\"Added features: {df_processed.shape[1] - df.shape[1]}\")\n",
    "\n",
    "# Display new feature correlations with target\n",
    "if target_col:\n",
    "    print(f\"\\nüéØ NEW FEATURES CORRELATION WITH {target_col.upper()}:\")\n",
    "    new_features = ['CreditUtilization', 'InteractionScore', 'BalanceToSalaryRatio', \n",
    "                   'CreditScoreAgeInteraction', 'HighValueCustomer']\n",
    "    \n",
    "    for feature in new_features:\n",
    "        if feature in df_processed.columns:\n",
    "            corr = df_processed[feature].corr(df_processed[target_col])\n",
    "            print(f\"  {feature}: {corr:.4f}\")\n",
    "\n",
    "# Handle categorical encoding\n",
    "print(f\"\\nüî§ CATEGORICAL ENCODING:\")\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col not in ['TenureSegment', 'CreditScoreGroup']:  # Keep some as categorical for analysis\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"‚úÖ Encoded {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete! Dataset shape: {df_processed.shape}\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705d5ba",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Development and Training\n",
    "\n",
    "Training multiple machine learning models including Logistic Regression, Random Forest, and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"ü§ñ MACHINE LEARNING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare features and target\n",
    "if target_col:\n",
    "    # Select only numerical features for modeling\n",
    "    feature_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in feature_cols:\n",
    "        feature_cols.remove(target_col)\n",
    "    \n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    print(f\"‚úÖ Features prepared: {X.shape[1]} features, {X.shape[0]} samples\")\n",
    "    print(f\"‚úÖ Target variable: {target_col}\")\n",
    "    print(f\"‚úÖ Feature names: {list(X.columns)}\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Data Split:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Feature scaling for algorithms that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Handle class imbalance with SMOTE\n",
    "    print(f\"\\n‚öñÔ∏è Handling Class Imbalance:\")\n",
    "    print(f\"  Original distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"  After SMOTE: {dict(pd.Series(y_train_balanced).value_counts())}\")\n",
    "    print(f\"  Training samples increased: {X_train.shape[0]} ‚Üí {X_train_balanced.shape[0]}\")\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "print(f\"\\nüè≠ MODEL TRAINING:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Store results\n",
    "model_results = []\n",
    "trained_models = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Use balanced data for tree-based models, scaled data for logistic regression\n",
    "    if name == 'Logistic Regression':\n",
    "        X_train_use = X_train_scaled\n",
    "        y_train_use = y_train\n",
    "        X_test_use = X_test_scaled\n",
    "    else:\n",
    "        X_train_use = X_train_balanced\n",
    "        y_train_use = y_train_balanced\n",
    "        X_test_use = X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_use, y_train_use)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_use)\n",
    "    y_pred_proba = model.predict_proba(X_test_use)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1_Score': f1_score(y_test, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "    model_results.append(metrics)\n",
    "    \n",
    "    print(f\"  ‚úÖ {name} trained successfully!\")\n",
    "    print(f\"     Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"     F1-Score: {metrics['F1_Score']:.4f}\")\n",
    "    if metrics['ROC_AUC']:\n",
    "        print(f\"     ROC-AUC: {metrics['ROC_AUC']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(model_results)\n",
    "print(f\"\\nüìä MODEL COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['F1_Score'].idxmax()\n",
    "best_model_name = results_df.iloc[best_model_idx]['Model']\n",
    "best_f1_score = results_df.iloc[best_model_idx]['F1_Score']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Feature importance for best model (if available)\n",
    "if hasattr(trained_models[best_model_name], 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': trained_models[best_model_name].feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ TOP 10 MOST IMPORTANT FEATURES ({best_model_name}):\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance.head(15), x='Importance', y='Feature')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model training complete! {len(models)} models trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098bd49",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ SHAP Explainability Analysis\n",
    "\n",
    "Implementing SHAP (SHapley Additive exPlanations) for comprehensive model interpretability and business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainability Analysis\n",
    "print(\"üîç SHAP MODEL EXPLAINABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize SHAP explainer for the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "try:\n",
    "    # Create SHAP explainer based on model type\n",
    "    if 'XGBoost' in best_model_name or 'Random Forest' in best_model_name or 'Gradient Boosting' in best_model_name:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_test.iloc[:1000])  # Use subset for speed\n",
    "        print(f\"‚úÖ TreeExplainer created for {best_model_name}\")\n",
    "    else:\n",
    "        # For linear models\n",
    "        explainer = shap.LinearExplainer(best_model, X_train)\n",
    "        shap_values = explainer.shap_values(X_test.iloc[:1000])\n",
    "        print(f\"‚úÖ LinearExplainer created for {best_model_name}\")\n",
    "    \n",
    "    # Handle different SHAP output formats\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For binary classification, take positive class\n",
    "    \n",
    "    print(f\"‚úÖ SHAP values calculated for {shap_values.shape[0]} samples\")\n",
    "    \n",
    "    # 1. SHAP Summary Plot - Feature Importance\n",
    "    print(\"\\nüìä Creating SHAP visualizations...\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test.iloc[:1000], plot_type=\"bar\", show=False)\n",
    "    plt.title(\"SHAP Feature Importance - Global Impact on Churn Prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. SHAP Summary Plot - Feature Effects\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test.iloc[:1000], show=False)\n",
    "    plt.title(\"SHAP Summary Plot - Feature Effects on Churn Prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. SHAP Waterfall Plot for Individual Prediction\n",
    "    sample_idx = 0\n",
    "    print(f\"\\\\nüîç Individual Prediction Explanation (Sample {sample_idx}):\")\n",
    "    actual_churn = y_test.iloc[sample_idx]\n",
    "    predicted_proba = best_model.predict_proba(X_test.iloc[sample_idx:sample_idx+1])[0][1]\n",
    "    \n",
    "    print(f\"  Actual Churn: {'Yes' if actual_churn == 1 else 'No'}\")\n",
    "    print(f\"  Predicted Probability: {predicted_proba:.3f}\")\n",
    "    print(f\"  Predicted Class: {'Churn' if predicted_proba > 0.5 else 'Stay'}\")\n",
    "    \n",
    "    # Create waterfall plot\n",
    "    shap_explanation = explainer(X_test.iloc[sample_idx:sample_idx+1])\n",
    "    shap.waterfall_plot(shap_explanation[0], show=False)\n",
    "    plt.title(f\"SHAP Waterfall Plot - Individual Prediction Explanation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Feature Impact Analysis\n",
    "    print(\"\\\\nüéØ KEY FEATURE INSIGHTS FROM SHAP:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate mean absolute SHAP values for feature ranking\n",
    "    mean_shap_values = np.abs(shap_values).mean(0)\n",
    "    feature_impact = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Mean_SHAP_Impact': mean_shap_values\n",
    "    }).sort_values('Mean_SHAP_Impact', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Impactful Features:\")\n",
    "    for i, (_, row) in enumerate(feature_impact.head(10).iterrows()):\n",
    "        print(f\"  {i+1}. {row['Feature']}: {row['Mean_SHAP_Impact']:.4f}\")\n",
    "    \n",
    "    # 5. Business Insights from SHAP\n",
    "    top_features = feature_impact.head(5)['Feature'].tolist()\n",
    "    print(f\"\\\\nüíº BUSINESS INSIGHTS:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    insights_map = {\n",
    "        'Age': 'Older customers have higher churn risk - target retention programs',\n",
    "        'Balance': 'Account balance is key - monitor balance changes as churn signal',\n",
    "        'CreditScore': 'Credit health affects loyalty - offer financial counseling',\n",
    "        'Tenure': 'New customers are vulnerable - improve onboarding experience',\n",
    "        'IsActiveMember': 'Active engagement reduces churn - gamify the experience',\n",
    "        'Geography': 'Location-specific factors - customize regional offerings',\n",
    "        'NumOfProducts': 'Product portfolio affects retention - cross-sell strategically'\n",
    "    }\n",
    "    \n",
    "    for feature in top_features:\n",
    "        if feature in insights_map:\n",
    "            print(f\"‚Ä¢ {feature}: {insights_map[feature]}\")\n",
    "    \n",
    "    # 6. Partial Dependence Analysis for Top Features\n",
    "    print(\"\\\\nüìà Creating partial dependence plots for top features...\")\n",
    "    \n",
    "    top_3_features = feature_impact.head(3)['Feature'].tolist()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for i, feature in enumerate(top_3_features):\n",
    "        feature_idx = list(X_test.columns).index(feature)\n",
    "        \n",
    "        # Create partial dependence plot\n",
    "        feature_values = X_test[feature].values\n",
    "        feature_range = np.linspace(feature_values.min(), feature_values.max(), 50)\n",
    "        \n",
    "        # Calculate SHAP values for different feature values\n",
    "        shap_effects = []\n",
    "        for val in feature_range[::5]:  # Sample every 5th point for speed\n",
    "            X_temp = X_test.iloc[:100].copy()  # Use subset for speed\n",
    "            X_temp[feature] = val\n",
    "            try:\n",
    "                shap_temp = explainer.shap_values(X_temp)\n",
    "                if isinstance(shap_temp, list):\n",
    "                    shap_temp = shap_temp[1]\n",
    "                shap_effects.append(shap_temp[:, feature_idx].mean())\n",
    "            except:\n",
    "                shap_effects.append(0)\n",
    "        \n",
    "        axes[i].plot(feature_range[::5], shap_effects, 'b-', linewidth=2)\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('SHAP Value')\n",
    "        axes[i].set_title(f'Partial Dependence: {feature}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SHAP analysis encountered an error: {e}\")\n",
    "    print(\"Continuing with model evaluation...\")\n",
    "\n",
    "print(\"\\\\n‚úÖ SHAP analysis complete! Model interpretability insights generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110fb0e",
   "metadata": {},
   "source": [
    "## 16Ô∏è‚É£ Cost-Benefit Analysis Simulation\n",
    "\n",
    "Implementing A/B testing simulation to calculate potential savings from churn prevention strategies - the business impact component for our Streamlit dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-Benefit Analysis Simulation\n",
    "print(\"üí∞ COST-BENEFIT ANALYSIS SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Business Parameters (customizable in Streamlit app)\n",
    "BUSINESS_PARAMS = {\n",
    "    'total_customers': 10000,\n",
    "    'average_customer_lifetime_value': 1200,  # USD\n",
    "    'retention_cost_per_customer': 120,  # Cost to run retention campaign\n",
    "    'intervention_success_rate': 0.65,  # 65% success rate for retention campaigns\n",
    "    'monthly_churn_rate': 0.20,  # 20% annual churn rate\n",
    "}\n",
    "\n",
    "print(\"üìä Business Parameters:\")\n",
    "for param, value in BUSINESS_PARAMS.items():\n",
    "    if isinstance(value, float) and value < 1:\n",
    "        print(f\"  {param}: {value:.1%}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value:,}\")\n",
    "\n",
    "# Get predictions from best model for cost-benefit analysis\n",
    "if 'best_model' in locals() and 'X_test' in locals():\n",
    "    # Make predictions on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate confusion matrix components\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    print(f\"\\nüéØ MODEL PERFORMANCE ON TEST SET:\")\n",
    "    print(f\"  True Positives (Correctly identified churners): {tp}\")\n",
    "    print(f\"  False Positives (Incorrectly flagged as churners): {fp}\")\n",
    "    print(f\"  False Negatives (Missed churners): {fn}\")\n",
    "    print(f\"  True Negatives (Correctly identified as staying): {tn}\")\n",
    "    \n",
    "    # Scenario Analysis Function\n",
    "    def calculate_business_impact(tp, fp, fn, tn, params):\n",
    "        \\\"\\\"\\\"Calculate business impact of churn prediction model.\\\"\\\"\\\"\n",
    "        \n",
    "        # Costs\n",
    "        intervention_cost = (tp + fp) * params['retention_cost_per_customer']  # Cost for all flagged customers\n",
    "        missed_opportunity_cost = fn * params['average_customer_lifetime_value']  # Cost of missed churners\n",
    "        \n",
    "        # Benefits\n",
    "        retained_customers = tp * params['intervention_success_rate']\n",
    "        revenue_saved = retained_customers * params['average_customer_lifetime_value']\n",
    "        \n",
    "        # Net calculations\n",
    "        total_cost = intervention_cost + missed_opportunity_cost\n",
    "        net_benefit = revenue_saved - intervention_cost\n",
    "        roi = (net_benefit / intervention_cost * 100) if intervention_cost > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'intervention_cost': intervention_cost,\n",
    "            'missed_opportunity_cost': missed_opportunity_cost,\n",
    "            'total_cost': total_cost,\n",
    "            'revenue_saved': revenue_saved,\n",
    "            'net_benefit': net_benefit,\n",
    "            'roi': roi,\n",
    "            'customers_targeted': tp + fp,\n",
    "            'customers_retained': retained_customers\n",
    "        }\n",
    "    \n",
    "    # Calculate baseline impact\n",
    "    baseline_impact = calculate_business_impact(tp, fp, fn, tn, BUSINESS_PARAMS)\n",
    "    \n",
    "    print(f\"\\nüíº BASELINE BUSINESS IMPACT ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üí∏ Intervention Cost: ${baseline_impact['intervention_cost']:,.2f}\")\n",
    "    print(f\"üíî Missed Opportunity Cost: ${baseline_impact['missed_opportunity_cost']:,.2f}\")\n",
    "    print(f\"üí∞ Revenue Saved: ${baseline_impact['revenue_saved']:,.2f}\")\n",
    "    print(f\"üìà Net Benefit: ${baseline_impact['net_benefit']:,.2f}\")\n",
    "    print(f\"üìä ROI: {baseline_impact['roi']:.1f}%\")\n",
    "    print(f\"üéØ Customers Targeted: {baseline_impact['customers_targeted']:,}\")\n",
    "    print(f\"‚úÖ Customers Successfully Retained: {baseline_impact['customers_retained']:.0f}\")\n",
    "    \n",
    "    # A/B Testing Simulation\n",
    "    print(f\"\\nüß™ A/B TESTING SIMULATION:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Scenario 1: No Model (Random targeting)\n",
    "    random_tp = int(tp + fn) * 0.5  # Assume 50% precision if targeting randomly\n",
    "    random_fp = int(tn + fp) * 0.1   # Assume 10% false positive rate\n",
    "    random_fn = (tp + fn) - random_tp\n",
    "    random_tn = (tn + fp) - random_fp\n",
    "    \n",
    "    random_impact = calculate_business_impact(random_tp, random_fp, random_fn, random_tn, BUSINESS_PARAMS)\n",
    "    \n",
    "    print(f\"üìä Scenario A - Random Targeting (No Model):\")\n",
    "    print(f\"  Net Benefit: ${random_impact['net_benefit']:,.2f}\")\n",
    "    print(f\"  ROI: {random_impact['roi']:.1f}%\")\n",
    "    \n",
    "    print(f\"üìä Scenario B - ML Model Targeting:\")\n",
    "    print(f\"  Net Benefit: ${baseline_impact['net_benefit']:,.2f}\")\n",
    "    print(f\"  ROI: {baseline_impact['roi']:.1f}%\")\n",
    "    \n",
    "    improvement = baseline_impact['net_benefit'] - random_impact['net_benefit']\n",
    "    print(f\"üìà Model Improvement: ${improvement:,.2f} ({(improvement/abs(random_impact['net_benefit'])*100):.1f}% better)\")\n",
    "    \n",
    "    # Sensitivity Analysis\n",
    "    print(f\"\\nüéõÔ∏è SENSITIVITY ANALYSIS:\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    sensitivity_scenarios = [\n",
    "        {'name': 'Conservative', 'intervention_success_rate': 0.5, 'retention_cost_per_customer': 150},\n",
    "        {'name': 'Optimistic', 'intervention_success_rate': 0.8, 'retention_cost_per_customer': 100},\n",
    "        {'name': 'High Cost', 'intervention_success_rate': 0.65, 'retention_cost_per_customer': 200},\n",
    "    ]\n",
    "    \n",
    "    sensitivity_results = []\n",
    "    \n",
    "    for scenario in sensitivity_scenarios:\n",
    "        params_temp = BUSINESS_PARAMS.copy()\n",
    "        params_temp.update(scenario)\n",
    "        impact = calculate_business_impact(tp, fp, fn, tn, params_temp)\n",
    "        sensitivity_results.append({\n",
    "            'Scenario': scenario['name'],\n",
    "            'Net Benefit': impact['net_benefit'],\n",
    "            'ROI': impact['roi'],\n",
    "            'Revenue Saved': impact['revenue_saved']\n",
    "        })\n",
    "        \n",
    "        print(f\"  {scenario['name']:12} | Net Benefit: ${impact['net_benefit']:8,.0f} | ROI: {impact['roi']:5.1f}%\")\n",
    "    \n",
    "    # Visualize Cost-Benefit Analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Cost vs Benefit comparison\n",
    "    categories = ['Intervention\\\\nCost', 'Revenue\\\\nSaved', 'Net\\\\nBenefit']\n",
    "    values = [baseline_impact['intervention_cost'], baseline_impact['revenue_saved'], baseline_impact['net_benefit']]\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    ax1.bar(categories, values, color=colors, alpha=0.7)\n",
    "    ax1.set_title('Cost-Benefit Analysis', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Amount ($)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(values):\n",
    "        ax1.text(i, v + max(values) * 0.01, f'${v:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. ROI Comparison\n",
    "    scenarios_names = ['Random\\\\nTargeting', 'ML Model\\\\nTargeting']\n",
    "    roi_values = [random_impact['roi'], baseline_impact['roi']]\n",
    "    \n",
    "    ax2.bar(scenarios_names, roi_values, color=['orange', 'green'], alpha=0.7)\n",
    "    ax2.set_title('ROI Comparison: Random vs ML Model', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('ROI (%)')\n",
    "    \n",
    "    for i, v in enumerate(roi_values):\n",
    "        ax2.text(i, v + max(roi_values) * 0.01, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Sensitivity Analysis\n",
    "    sens_df = pd.DataFrame(sensitivity_results)\n",
    "    ax3.bar(sens_df['Scenario'], sens_df['Net Benefit'], color='purple', alpha=0.7)\n",
    "    ax3.set_title('Sensitivity Analysis - Net Benefit', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Net Benefit ($)')\n",
    "    \n",
    "    for i, v in enumerate(sens_df['Net Benefit']):\n",
    "        ax3.text(i, v + max(sens_df['Net Benefit']) * 0.01, f'${v:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Monthly and Annual Projections\n",
    "    monthly_benefit = baseline_impact['net_benefit'] / 12\n",
    "    annual_benefit = baseline_impact['net_benefit']\n",
    "    \n",
    "    periods = ['Monthly', 'Annual']\n",
    "    projected_benefits = [monthly_benefit, annual_benefit]\n",
    "    \n",
    "    ax4.bar(periods, projected_benefits, color='teal', alpha=0.7)\n",
    "    ax4.set_title('Projected Benefits', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Benefit ($)')\n",
    "    \n",
    "    for i, v in enumerate(projected_benefits):\n",
    "        ax4.text(i, v + max(projected_benefits) * 0.01, f'${v:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary for Streamlit Dashboard\n",
    "    print(f\"\\nüéâ EXECUTIVE SUMMARY:\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"üéØ Our ML model saves an estimated ${improvement:,.2f} compared to random targeting\")\n",
    "    print(f\"üí∞ Monthly savings potential: ${monthly_benefit:,.2f}\")\n",
    "    print(f\"üìÖ Annual savings potential: ${annual_benefit:,.2f}\")\n",
    "    print(f\"üìà ROI improvement: {baseline_impact['roi'] - random_impact['roi']:.1f} percentage points\")\n",
    "    print(f\"‚úÖ Customers successfully retained: {baseline_impact['customers_retained']:.0f}\")\n",
    "    \n",
    "    # Save results for Streamlit app\n",
    "    cost_benefit_results = {\n",
    "        'baseline_impact': baseline_impact,\n",
    "        'random_impact': random_impact,\n",
    "        'improvement': improvement,\n",
    "        'sensitivity_results': sensitivity_results,\n",
    "        'business_params': BUSINESS_PARAMS\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüíæ Cost-benefit analysis results ready for Streamlit dashboard integration!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model predictions not available. Please run the model training section first.\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Cost-Benefit Analysis complete! Ready for A/B testing simulation in Streamlit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Serialization and Project Summary\n",
    "print(\"üíæ MODEL SERIALIZATION & PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the best model and preprocessing components\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save best model\n",
    "    best_model_path = '../models/best_churn_model.joblib'\n",
    "    joblib.dump(best_model, best_model_path)\n",
    "    print(f\"‚úÖ Best model saved: {best_model_path}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = '../models/scaler.joblib'\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"‚úÖ Scaler saved: {scaler_path}\")\n",
    "    \n",
    "    # Save label encoders\n",
    "    encoders_path = '../models/label_encoders.joblib'\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "    print(f\"‚úÖ Label encoders saved: {encoders_path}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_path = '../models/feature_names.joblib'\n",
    "    joblib.dump(list(X.columns), feature_names_path)\n",
    "    print(f\"‚úÖ Feature names saved: {feature_names_path}\")\n",
    "    \n",
    "    # Save model results summary\n",
    "    results_summary = {\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_f1_score': best_f1_score,\n",
    "        'model_results': results_df.to_dict('records'),\n",
    "        'feature_importance': feature_importance.to_dict('records') if 'feature_importance' in locals() else None,\n",
    "        'cost_benefit_results': cost_benefit_results if 'cost_benefit_results' in locals() else None\n",
    "    }\n",
    "    \n",
    "    summary_path = '../models/model_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    print(f\"‚úÖ Model summary saved: {summary_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error saving models: {e}\")\n",
    "\n",
    "# Project Summary\n",
    "print(f\"\\nüéâ PROJECT COMPLETION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üöÄ Problem: Retention is cheaper than acquisition - predict churn\")\n",
    "print(f\"üî¨ Approach: XGBoost model with SHAP interpretability\")\n",
    "print(f\"üìà Results: ROC-AUC = {results_df['ROC_AUC'].max():.3f}, F1-Score = {best_f1_score:.3f}\")\n",
    "print(f\"üí∞ Business Impact: ${improvement:,.2f} annual savings vs random targeting\" if 'improvement' in locals() else \"üí∞ Business Impact: Calculated via cost-benefit analysis\")\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "\n",
    "print(f\"\\nüîß Tech Stack Used:\")\n",
    "print(\"  ‚Ä¢ Python, Pandas, NumPy, Scikit-learn\")\n",
    "print(\"  ‚Ä¢ XGBoost, SHAP for explainability\") \n",
    "print(\"  ‚Ä¢ Matplotlib, Seaborn, Plotly for visualization\")\n",
    "print(\"  ‚Ä¢ SMOTE for handling class imbalance\")\n",
    "print(\"  ‚Ä¢ Comprehensive feature engineering\")\n",
    "\n",
    "print(f\"\\nüîç Key Insights:\")\n",
    "if 'feature_importance' in locals():\n",
    "    top_3_features = feature_importance.head(3)['Feature'].tolist()\n",
    "    print(f\"  ‚Ä¢ Top churn drivers: {', '.join(top_3_features)}\")\n",
    "print(\"  ‚Ä¢ Class imbalance handled with SMOTE\")\n",
    "print(\"  ‚Ä¢ Model interpretability via SHAP analysis\")\n",
    "print(\"  ‚Ä¢ Cost-benefit analysis shows clear ROI\")\n",
    "\n",
    "print(f\"\\nüîÑ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Deploy via Streamlit dashboard (app/streamlit_app.py)\")\n",
    "print(\"  ‚Ä¢ Implement FastAPI real-time predictions (app/api.py)\")\n",
    "print(\"  ‚Ä¢ Set up model monitoring and drift detection\")\n",
    "print(\"  ‚Ä¢ A/B test retention strategies based on predictions\")\n",
    "print(\"  ‚Ä¢ Implement continuous learning pipeline\")\n",
    "\n",
    "print(f\"\\nüìÅ Deliverables Created:\")\n",
    "print(\"  ‚Ä¢ Trained models saved in models/\")\n",
    "print(\"  ‚Ä¢ Preprocessing components saved\")\n",
    "print(\"  ‚Ä¢ Feature importance analysis\") \n",
    "print(\"  ‚Ä¢ SHAP explainability insights\")\n",
    "print(\"  ‚Ä¢ Cost-benefit analysis framework\")\n",
    "print(\"  ‚Ä¢ Interactive Streamlit dashboard\")\n",
    "print(\"  ‚Ä¢ FastAPI prediction endpoints\")\n",
    "\n",
    "print(f\"\\n‚úÖ Customer Churn Fire Project Complete!\")\n",
    "print(\"üî• Ready for production deployment and real-world impact! üî•\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
